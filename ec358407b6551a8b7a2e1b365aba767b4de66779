---
type: learnings
timestamp: '2025-12-26T20:40:01.710809+00:00'
summary: 'Issue #28 validated: Stop hook uses push_notes_to_remote() which causes
  race conditions'
---

Issue #28 validated: Stop hook uses push_notes_to_remote() which causes race conditions
## Summary
Multi-worktree notes conflicts occur because stop_handler.py:482 calls `push_notes_to_remote()` directly instead of the proper `sync_notes_with_remote(push=True)` method.

## Code Analysis
- `push_notes_to_remote()` at git_ops.py:1222 pushes without fetching first
- `sync_notes_with_remote()` at git_ops.py:1238 implements correct fetch→merge→push workflow
- Fix is straightforward: replace one method call with another

## Related Files
- src/git_notes_memory/git_ops.py:1222-1281
- src/git_notes_memory/hooks/stop_handler.py:473-487

---
type: learnings
timestamp: '2025-12-26T20:40:01.726856+00:00'
summary: Telemetry export is batched at session end, not during session
---

Telemetry export is batched at session end, not during session
## Summary
Metrics, traces, and logs are collected during the session but only exported to OTLP when the Stop hook runs at session termination. This is intentional batch export design.

## Context
- `stop_handler.py:505-597` handles all OTLP exports
- Metrics accumulate in-memory during session
- `export_metrics_if_configured()` called only at session end
- No mid-session push to reduce network overhead

## Related Files
- src/git_notes_memory/hooks/stop_handler.py:505-597

---
type: learnings
timestamp: '2025-12-26T20:40:01.746240+00:00'
summary: UserPromptSubmit hook runs on USER prompts, not assistant output
---

UserPromptSubmit hook runs on USER prompts, not assistant output
## Summary
The signal detector runs on UserPromptSubmit hook which only sees user input. Memory blocks in assistant responses (like mine) are NOT captured by this hook.

## Context
- `user_prompt_handler.py` processes user prompts
- Detection happens in user's input text, not Claude's responses
- Memory blocks I write in responses go uncaptured

## Related Files
- src/git_notes_memory/hooks/user_prompt_handler.py
- src/git_notes_memory/hooks/signal_detector.py:183-190

---
type: learnings
timestamp: '2025-12-26T20:47:19.330840+00:00'
summary: Process isolation is the root cause of empty metrics
---

Process isolation is the root cause of empty metrics
## Summary
The `/memory:metrics` command shows empty output because each slash command runs in an **isolated Python process**. Metrics collected during one operation die with that process before the next command can see them.

## Architecture Details

### The In-Memory Singleton Problem

```
┌─────────────────────────────────────────────────────────────────────┐
│  /memory:capture "test"                                              │
│  ┌─────────────────┐                                                │
│  │ Python Process A │                                               │
│  │  MetricsCollector (empty → collects metrics → pushes to OTLP)   │
│  └─────────────────┘                                                │
│           ↓ process exits, metrics die                              │
└─────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────┐
│  /memory:metrics                                                     │
│  ┌─────────────────┐                                                │
│  │ Python Process B │  ← COMPLETELY SEPARATE PROCESS                │
│  │  MetricsCollector() ← FRESH, EMPTY                               │
│  └─────────────────┘                                                │
│           ↓ shows nothing because no operations ran in THIS process │
└─────────────────────────────────────────────────────────────────────┘
```

### The Push Model (OTLP)

The code DOES push metrics to OTLP—but only at specific points:

| Location | When | What's Pushed |
|----------|------|---------------|
| `capture.py:812` | After each capture | Capture metrics |
| `stop_handler.py:541` | On session Stop | All session metrics + traces |
| `hook_utils.py:572` | Helper function | Metrics collected so far |

**BUT** this only works if `MEMORY_PLUGIN_OTLP_ENDPOINT` is set DURING those operations.

## Related Files
- src/git_notes_memory/observability/metrics.py:485-502 (singleton pattern)
- src/git_notes_memory/observability/exporters/otlp.py:552-565 (push function)
- src/git_notes_memory/hooks/stop_handler.py:541 (Stop hook export)

---
type: learnings
timestamp: '2025-12-26T20:47:19.376330+00:00'
summary: 'Now I understand. Let me check the OTLP export flow:'
---

Now I understand. Let me check the OTLP export flow:

---
type: learnings
timestamp: '2025-12-26T20:56:47.344796+00:00'
summary: Process isolation is the root cause of empty metrics
---

Process isolation is the root cause of empty metrics
## Summary
The `/memory:metrics` command shows empty output because each slash command runs in an **isolated Python process**. Metrics collected during one operation die with that process before the next command can see them.

## Architecture Details

### The In-Memory Singleton Problem

```
┌─────────────────────────────────────────────────────────────────────┐
│  /memory:capture "test"                                              │
│  ┌─────────────────┐                                                │
│  │ Python Process A │                                               │
│  │  MetricsCollector (empty → collects metrics → pushes to OTLP)   │
│  └─────────────────┘                                                │
│           ↓ process exits, metrics die                              │
└─────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────┐
│  /memory:metrics                                                     │
│  ┌─────────────────┐                                                │
│  │ Python Process B │  ← COMPLETELY SEPARATE PROCESS                │
│  │  MetricsCollector() ← FRESH, EMPTY                               │
│  └─────────────────┘                                                │
│           ↓ shows nothing because no operations ran in THIS process │
└─────────────────────────────────────────────────────────────────────┘
```

### The Push Model (OTLP)

The code DOES push metrics to OTLP—but only at specific points:

| Location | When | What's Pushed |
|----------|------|---------------|
| `capture.py:812` | After each capture | Capture metrics |
| `stop_handler.py:541` | On session Stop | All session metrics + traces |
| `hook_utils.py:572` | Helper function | Metrics collected so far |

**BUT** this only works if `MEMORY_PLUGIN_OTLP_ENDPOINT` is set DURING those operations.

## Related Files
- src/git_notes_memory/observability/metrics.py:485-502 (singleton pattern)
- src/git_notes_memory/observability/exporters/otlp.py:552-565 (push function)
- src/git_notes_memory/hooks/stop_handler.py:541 (Stop hook export)

---
type: learnings
timestamp: '2025-12-26T20:56:47.363641+00:00'
summary: OTel Collector's prometheus exporter doesn't accumulate OTLP pushed metrics
---

OTel Collector's prometheus exporter doesn't accumulate OTLP pushed metrics
## Summary
The `prometheus` exporter in OTel Collector exposes metrics for scraping but **doesn't retain** metrics pushed via OTLP. They flow through and are gone.

## Architecture Issue
```
OTLP Push → OTel Collector → prometheusremotewrite exporter → Prometheus /api/v1/write
                           ↘ prometheus exporter (port 8889) → EMPTY (no accumulation)
```

## Root Cause
1. Prometheus config has `remote_write: []` (empty)
2. Prometheus needs `--web.enable-remote-write-receiver` flag
3. The `prometheus` exporter on 8889 is empty because it doesn't accumulate pushed data

## Related Files
- docker/otel-collector-config.yaml:47-48 (prometheusremotewrite)
- docker/prometheus/prometheus.yml (remote_write config)

---
type: learnings
timestamp: '2025-12-26T21:03:08.089114+00:00'
summary: Process isolation is the root cause of empty metrics
---

Process isolation is the root cause of empty metrics
## Summary
The `/memory:metrics` command shows empty output because each slash command runs in an **isolated Python process**. Metrics collected during one operation die with that process before the next command can see them.

## Architecture Details

### The In-Memory Singleton Problem

```
┌─────────────────────────────────────────────────────────────────────┐
│  /memory:capture "test"                                              │
│  ┌─────────────────┐                                                │
│  │ Python Process A │                                               │
│  │  MetricsCollector (empty → collects metrics → pushes to OTLP)   │
│  └─────────────────┘                                                │
│           ↓ process exits, metrics die                              │
└─────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────┐
│  /memory:metrics                                                     │
│  ┌─────────────────┐                                                │
│  │ Python Process B │  ← COMPLETELY SEPARATE PROCESS                │
│  │  MetricsCollector() ← FRESH, EMPTY                               │
│  └─────────────────┘                                                │
│           ↓ shows nothing because no operations ran in THIS process │
└─────────────────────────────────────────────────────────────────────┘
```

### The Push Model (OTLP)

The code DOES push metrics to OTLP—but only at specific points:

| Location | When | What's Pushed |
|----------|------|---------------|
| `capture.py:812` | After each capture | Capture metrics |
| `stop_handler.py:541` | On session Stop | All session metrics + traces |
| `hook_utils.py:572` | Helper function | Metrics collected so far |

**BUT** this only works if `MEMORY_PLUGIN_OTLP_ENDPOINT` is set DURING those operations.

## Related Files
- src/git_notes_memory/observability/metrics.py:485-502 (singleton pattern)
- src/git_notes_memory/observability/exporters/otlp.py:552-565 (push function)
- src/git_notes_memory/hooks/stop_handler.py:541 (Stop hook export)

---
type: learnings
timestamp: '2025-12-26T21:03:08.108870+00:00'
summary: OTel Collector's prometheus exporter doesn't accumulate OTLP pushed metrics
---

OTel Collector's prometheus exporter doesn't accumulate OTLP pushed metrics
## Summary
The `prometheus` exporter in OTel Collector exposes metrics for scraping but **doesn't retain** metrics pushed via OTLP. They flow through and are gone.

## Architecture Issue
```
OTLP Push → OTel Collector → prometheusremotewrite exporter → Prometheus /api/v1/write
                           ↘ prometheus exporter (port 8889) → EMPTY (no accumulation)
```

## Root Cause
1. Prometheus config has `remote_write: []` (empty)
2. Prometheus needs `--web.enable-remote-write-receiver` flag
3. The `prometheus` exporter on 8889 is empty because it doesn't accumulate pushed data

## Related Files
- docker/otel-collector-config.yaml:47-48 (prometheusremotewrite)
- docker/prometheus/prometheus.yml (remote_write config)

---
type: learnings
timestamp: '2025-12-26T21:08:39.587051+00:00'
summary: '▶ learned ─────────────────────────────────────

  Process isolation is the root cause of empty metr...'
tags:
- auto-captured
- pre-compact
---

Process isolation is the root cause of empty metrics
## Summary
The `/memory:metrics` command shows empty output because each slash command runs in an **isolated Python process**. Metrics collected during one operation die with that process before the next command can see them.

## Architecture Details

### The In-Memory Singleton Problem

```
┌─────────────────────────────────────────────────────────────────────┐
│  /memory:capture "test"                                              │
│  ┌─────────────────┐                                                │
│  │ Python Process A │                                               │
│  │  MetricsCollector (empty → collects metrics → pushes to OTLP)   │
│  └─────────────────┘                                                │
│           ↓ process exits, metrics die                              │
└─────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────┐
│  /memory:metrics                                                     │
│  ┌─────────────────┐                                                │
│  │ Python Process B │  ← COMPLETELY SEPARATE PROCESS                │
│  │  MetricsCollector() ← FRESH, EMPTY                               │
│  └─────────────────┘                                                │
│           ↓ shows nothing because no operations ran in THIS process │
└─────────────────────────────────────────────────────────────────────┘
```

### The Push Model (OTLP)

The code DOES push metrics to OTLP—but only at specific points:

| Location | When | What's Pushed |
|----------|------|---------------|
| `capture.py:812` | After each capture | Capture metrics |
| `stop_handler.py:541` | On session Stop | All session metrics + traces |
| `hook_utils.py:572` | Helper function | Metrics collected so far |

**BUT** this only works if `MEMORY_PLUGIN_OTLP_ENDPOINT` is set DURING those operations.

## Related Files
- src/git_notes_memory/observability/metrics.py:485-502 (singleton pattern)
- src/git_notes_memory/observability/exporters/otlp.py:552-565 (push function)
- src/git_notes_memory/hooks/stop_handler.py:541 (Stop hook export)

---
type: learnings
timestamp: '2025-12-26T21:08:39.629100+00:00'
summary: '▶ learned ─────────────────────────────────────

  OTel Collector''s prometheus exporter doesn''t accu...'
tags:
- auto-captured
- pre-compact
---

OTel Collector's prometheus exporter doesn't accumulate OTLP pushed metrics
## Summary
The `prometheus` exporter in OTel Collector exposes metrics for scraping but **doesn't retain** metrics pushed via OTLP. They flow through and are gone.

## Architecture Issue
```
OTLP Push → OTel Collector → prometheusremotewrite exporter → Prometheus /api/v1/write
                           ↘ prometheus exporter (port 8889) → EMPTY (no accumulation)
```

## Root Cause
1. Prometheus config has `remote_write: []` (empty)
2. Prometheus needs `--web.enable-remote-write-receiver` flag
3. The `prometheus` exporter on 8889 is empty because it doesn't accumulate pushed data

## Related Files
- docker/otel-collector-config.yaml:47-48 (prometheusremotewrite)
- docker/prometheus/prometheus.yml (remote_write config)

---
type: learnings
timestamp: '2025-12-26T21:13:32.109972+00:00'
summary: test content
tags:
- explicit
- testing
---

test content

---
type: learnings
timestamp: '2025-12-26T21:13:52.446787+00:00'
summary: Process isolation is the root cause of empty metrics
---

Process isolation is the root cause of empty metrics
## Summary
The `/memory:metrics` command shows empty output because each slash command runs in an **isolated Python process**. Metrics collected during one operation die with that process before the next command can see them.

## Architecture Details

### The In-Memory Singleton Problem

```
┌─────────────────────────────────────────────────────────────────────┐
│  /memory:capture "test"                                              │
│  ┌─────────────────┐                                                │
│  │ Python Process A │                                               │
│  │  MetricsCollector (empty → collects metrics → pushes to OTLP)   │
│  └─────────────────┘                                                │
│           ↓ process exits, metrics die                              │
└─────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────┐
│  /memory:metrics                                                     │
│  ┌─────────────────┐                                                │
│  │ Python Process B │  ← COMPLETELY SEPARATE PROCESS                │
│  │  MetricsCollector() ← FRESH, EMPTY                               │
│  └─────────────────┘                                                │
│           ↓ shows nothing because no operations ran in THIS process │
└─────────────────────────────────────────────────────────────────────┘
```

### The Push Model (OTLP)

The code DOES push metrics to OTLP—but only at specific points:

| Location | When | What's Pushed |
|----------|------|---------------|
| `capture.py:812` | After each capture | Capture metrics |
| `stop_handler.py:541` | On session Stop | All session metrics + traces |
| `hook_utils.py:572` | Helper function | Metrics collected so far |

**BUT** this only works if `MEMORY_PLUGIN_OTLP_ENDPOINT` is set DURING those operations.

## Related Files
- src/git_notes_memory/observability/metrics.py:485-502 (singleton pattern)
- src/git_notes_memory/observability/exporters/otlp.py:552-565 (push function)
- src/git_notes_memory/hooks/stop_handler.py:541 (Stop hook export)

---
type: learnings
timestamp: '2025-12-26T21:13:52.466060+00:00'
summary: OTel Collector's prometheus exporter doesn't accumulate OTLP pushed metrics
---

OTel Collector's prometheus exporter doesn't accumulate OTLP pushed metrics
## Summary
The `prometheus` exporter in OTel Collector exposes metrics for scraping but **doesn't retain** metrics pushed via OTLP. They flow through and are gone.

## Architecture Issue
```
OTLP Push → OTel Collector → prometheusremotewrite exporter → Prometheus /api/v1/write
                           ↘ prometheus exporter (port 8889) → EMPTY (no accumulation)
```

## Root Cause
1. Prometheus config has `remote_write: []` (empty)
2. Prometheus needs `--web.enable-remote-write-receiver` flag
3. The `prometheus` exporter on 8889 is empty because it doesn't accumulate pushed data

## Related Files
- docker/otel-collector-config.yaml:47-48 (prometheusremotewrite)
- docker/prometheus/prometheus.yml (remote_write config)

---
type: learnings
timestamp: '2025-12-26T21:16:11.189770+00:00'
summary: Process isolation is the root cause of empty metrics
---

Process isolation is the root cause of empty metrics
## Summary
The `/memory:metrics` command shows empty output because each slash command runs in an **isolated Python process**. Metrics collected during one operation die with that process before the next command can see them.

## Architecture Details

### The In-Memory Singleton Problem

```
┌─────────────────────────────────────────────────────────────────────┐
│  /memory:capture "test"                                              │
│  ┌─────────────────┐                                                │
│  │ Python Process A │                                               │
│  │  MetricsCollector (empty → collects metrics → pushes to OTLP)   │
│  └─────────────────┘                                                │
│           ↓ process exits, metrics die                              │
└─────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────┐
│  /memory:metrics                                                     │
│  ┌─────────────────┐                                                │
│  │ Python Process B │  ← COMPLETELY SEPARATE PROCESS                │
│  │  MetricsCollector() ← FRESH, EMPTY                               │
│  └─────────────────┘                                                │
│           ↓ shows nothing because no operations ran in THIS process │
└─────────────────────────────────────────────────────────────────────┘
```

### The Push Model (OTLP)

The code DOES push metrics to OTLP—but only at specific points:

| Location | When | What's Pushed |
|----------|------|---------------|
| `capture.py:812` | After each capture | Capture metrics |
| `stop_handler.py:541` | On session Stop | All session metrics + traces |
| `hook_utils.py:572` | Helper function | Metrics collected so far |

**BUT** this only works if `MEMORY_PLUGIN_OTLP_ENDPOINT` is set DURING those operations.

## Related Files
- src/git_notes_memory/observability/metrics.py:485-502 (singleton pattern)
- src/git_notes_memory/observability/exporters/otlp.py:552-565 (push function)
- src/git_notes_memory/hooks/stop_handler.py:541 (Stop hook export)

---
type: learnings
timestamp: '2025-12-26T21:16:11.212273+00:00'
summary: OTel Collector's prometheus exporter doesn't accumulate OTLP pushed metrics
---

OTel Collector's prometheus exporter doesn't accumulate OTLP pushed metrics
## Summary
The `prometheus` exporter in OTel Collector exposes metrics for scraping but **doesn't retain** metrics pushed via OTLP. They flow through and are gone.

## Architecture Issue
```
OTLP Push → OTel Collector → prometheusremotewrite exporter → Prometheus /api/v1/write
                           ↘ prometheus exporter (port 8889) → EMPTY (no accumulation)
```

## Root Cause
1. Prometheus config has `remote_write: []` (empty)
2. Prometheus needs `--web.enable-remote-write-receiver` flag
3. The `prometheus` exporter on 8889 is empty because it doesn't accumulate pushed data

## Related Files
- docker/otel-collector-config.yaml:47-48 (prometheusremotewrite)
- docker/prometheus/prometheus.yml (remote_write config)

---
type: learnings
timestamp: '2025-12-26T21:18:10.148997+00:00'
summary: Process isolation is the root cause of empty metrics
---

Process isolation is the root cause of empty metrics
## Summary
The `/memory:metrics` command shows empty output because each slash command runs in an **isolated Python process**. Metrics collected during one operation die with that process before the next command can see them.

## Architecture Details

### The In-Memory Singleton Problem

```
┌─────────────────────────────────────────────────────────────────────┐
│  /memory:capture "test"                                              │
│  ┌─────────────────┐                                                │
│  │ Python Process A │                                               │
│  │  MetricsCollector (empty → collects metrics → pushes to OTLP)   │
│  └─────────────────┘                                                │
│           ↓ process exits, metrics die                              │
└─────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────┐
│  /memory:metrics                                                     │
│  ┌─────────────────┐                                                │
│  │ Python Process B │  ← COMPLETELY SEPARATE PROCESS                │
│  │  MetricsCollector() ← FRESH, EMPTY                               │
│  └─────────────────┘                                                │
│           ↓ shows nothing because no operations ran in THIS process │
└─────────────────────────────────────────────────────────────────────┘
```

### The Push Model (OTLP)

The code DOES push metrics to OTLP—but only at specific points:

| Location | When | What's Pushed |
|----------|------|---------------|
| `capture.py:812` | After each capture | Capture metrics |
| `stop_handler.py:541` | On session Stop | All session metrics + traces |
| `hook_utils.py:572` | Helper function | Metrics collected so far |

**BUT** this only works if `MEMORY_PLUGIN_OTLP_ENDPOINT` is set DURING those operations.

## Related Files
- src/git_notes_memory/observability/metrics.py:485-502 (singleton pattern)
- src/git_notes_memory/observability/exporters/otlp.py:552-565 (push function)
- src/git_notes_memory/hooks/stop_handler.py:541 (Stop hook export)

---
type: learnings
timestamp: '2025-12-26T21:18:10.169115+00:00'
summary: OTel Collector's prometheus exporter doesn't accumulate OTLP pushed metrics
---

OTel Collector's prometheus exporter doesn't accumulate OTLP pushed metrics
## Summary
The `prometheus` exporter in OTel Collector exposes metrics for scraping but **doesn't retain** metrics pushed via OTLP. They flow through and are gone.

## Architecture Issue
```
OTLP Push → OTel Collector → prometheusremotewrite exporter → Prometheus /api/v1/write
                           ↘ prometheus exporter (port 8889) → EMPTY (no accumulation)
```

## Root Cause
1. Prometheus config has `remote_write: []` (empty)
2. Prometheus needs `--web.enable-remote-write-receiver` flag
3. The `prometheus` exporter on 8889 is empty because it doesn't accumulate pushed data

## Related Files
- docker/otel-collector-config.yaml:47-48 (prometheusremotewrite)
- docker/prometheus/prometheus.yml (remote_write config)

---
type: learnings
timestamp: '2025-12-26T21:36:27.718778+00:00'
summary: test content
tags:
- explicit
- testing
---

test content

---
type: learnings
timestamp: '2025-12-26T22:19:21.605953+00:00'
summary: test content
tags:
- explicit
- testing
---

test content

---
type: learnings
timestamp: '2025-12-26T22:26:34.391202+00:00'
summary: test content
tags:
- explicit
- testing
---

test content

---
type: learnings
timestamp: '2025-12-26T22:27:04.908589+00:00'
summary: 'Now I understand the core problem. Let me check what metrics are actually
  being recorded:'
---

Now I understand the core problem. Let me check what metrics are actually being recorded:

---
type: learnings
timestamp: '2025-12-26T23:12:44.206053+00:00'
summary: 'Now I understand the core problem. Let me check what metrics are actually
  being recorded:'
---

Now I understand the core problem. Let me check what metrics are actually being recorded:

---
type: learnings
timestamp: '2025-12-26T23:14:48.116090+00:00'
summary: 'Now I understand the core problem. Let me check what metrics are actually
  being recorded:'
---

Now I understand the core problem. Let me check what metrics are actually being recorded:

---
type: learnings
timestamp: '2025-12-26T23:17:00.633112+00:00'
summary: 'Now I understand the core problem. Let me check what metrics are actually
  being recorded:'
---

Now I understand the core problem. Let me check what metrics are actually being recorded:

---
type: learnings
timestamp: '2025-12-26T23:31:59.866965+00:00'
summary: test content
tags:
- explicit
- testing
---

test content

---
type: learnings
timestamp: '2025-12-26T23:42:47.712694+00:00'
summary: 'Now I understand the core problem. Let me check what metrics are actually
  being recorded:'
---

Now I understand the core problem. Let me check what metrics are actually being recorded:

---
type: learnings
timestamp: '2025-12-27T01:12:36.251213+00:00'
summary: 'Now I understand the core problem. Let me check what metrics are actually
  being recorded:'
---

Now I understand the core problem. Let me check what metrics are actually being recorded:

---
type: learnings
timestamp: '2025-12-27T01:13:18.074704+00:00'
summary: 'Now I understand the core problem. Let me check what metrics are actually
  being recorded:'
---

Now I understand the core problem. Let me check what metrics are actually being recorded:

---
type: learnings
timestamp: '2025-12-27T01:14:18.148282+00:00'
summary: 'Now I understand the core problem. Let me check what metrics are actually
  being recorded:'
---

Now I understand the core problem. Let me check what metrics are actually being recorded:

---
type: learnings
timestamp: '2025-12-27T01:15:08.289933+00:00'
summary: 'Now I understand the core problem. Let me check what metrics are actually
  being recorded:'
---

Now I understand the core problem. Let me check what metrics are actually being recorded:

---
type: learnings
timestamp: '2025-12-27T01:15:27.458597+00:00'
summary: 'Now I understand the core problem. Let me check what metrics are actually
  being recorded:'
---

Now I understand the core problem. Let me check what metrics are actually being recorded:

---
type: learnings
timestamp: '2025-12-27T01:15:45.201299+00:00'
summary: 'Now I understand the core problem. Let me check what metrics are actually
  being recorded:'
---

Now I understand the core problem. Let me check what metrics are actually being recorded:

---
type: learnings
timestamp: '2025-12-27T01:28:51.506010+00:00'
summary: Telemetry pipeline works correctly but metrics appear empty until hooks execute
---

Telemetry pipeline works correctly but metrics appear empty until hooks execute
## Summary
The OTLP telemetry pipeline is fully functional. Metrics were appearing empty because few hooks had executed, not due to a configuration issue.

## Key Insights
- OTLP export from hooks works (http://localhost:4318)
- OTEL collector forwards to Prometheus via remote write
- Prometheus scrape interval is 15s - must wait for fresh data
- Metrics persist across hook invocations via ~/.local/share/memory-plugin/metrics_state.json
- Grafana dashboards are pre-configured at http://localhost:3000

## Related Files
- src/git_notes_memory/hooks/hook_utils.py:329-387 (timed_hook_execution context manager)
- src/git_notes_memory/observability/exporters/otlp.py:152-566 (OTLPExporter)
- src/git_notes_memory/observability/persistent_metrics.py:1-150 (persistence layer)

---
type: learnings
timestamp: '2025-12-27T12:31:20.042028+00:00'
summary: Telemetry pipeline works correctly but metrics appear empty until hooks execute
---

Telemetry pipeline works correctly but metrics appear empty until hooks execute
## Summary
The OTLP telemetry pipeline is fully functional. Metrics were appearing empty because few hooks had executed, not due to a configuration issue.

## Key Insights
- OTLP export from hooks works (http://localhost:4318)
- OTEL collector forwards to Prometheus via remote write
- Prometheus scrape interval is 15s - must wait for fresh data
- Metrics persist across hook invocations via ~/.local/share/memory-plugin/metrics_state.json
- Grafana dashboards are pre-configured at http://localhost:3000

## Related Files
- src/git_notes_memory/hooks/hook_utils.py:329-387 (timed_hook_execution context manager)
- src/git_notes_memory/observability/exporters/otlp.py:152-566 (OTLPExporter)
- src/git_notes_memory/observability/persistent_metrics.py:1-150 (persistence layer)

---
type: learnings
timestamp: '2025-12-27T12:31:52.810190+00:00'
summary: '▶ learned ─────────────────────────────────────

  Telemetry pipeline works correctly but metrics ap...'
tags:
- auto-captured
- pre-compact
---

Telemetry pipeline works correctly but metrics appear empty until hooks execute
## Summary
The OTLP telemetry pipeline is fully functional. Metrics were appearing empty because few hooks had executed, not due to a configuration issue.

## Key Insights
- OTLP export from hooks works (http://localhost:4318)
- OTEL collector forwards to Prometheus via remote write
- Prometheus scrape interval is 15s - must wait for fresh data
- Metrics persist across hook invocations via ~/.local/share/memory-plugin/metrics_state.json
- Grafana dashboards are pre-configured at http://localhost:3000

## Related Files
- src/git_notes_memory/hooks/hook_utils.py:329-387 (timed_hook_execution context manager)
- src/git_notes_memory/observability/exporters/otlp.py:152-566 (OTLPExporter)
- src/git_notes_memory/observability/persistent_metrics.py:1-150 (persistence layer)

---
type: learnings
timestamp: '2025-12-27T12:41:55.115286+00:00'
summary: test content
tags:
- explicit
- testing
---

test content
